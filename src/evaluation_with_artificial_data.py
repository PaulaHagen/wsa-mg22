import random
from collections import Counter
import re
import pandas as pd
from datetime import datetime, timedelta
from chat import chatgpt


def create_artificial_topic(research_field):
    '''
    Sends prompt to ChatGPT and returns an artificial topic within a given research field. 
    The topic is represented by its top 10 words, which are returned.

    Params:
     str: research_field
    
    Returns:
     list(str): artificial_topic
    '''

    '''
    artificial_topic =  [
        'recyclico', 'closed_loop', 'remanufacture', 'resource_efficiency', 
        'reverse_logistics', 'eco_design', 'material_recovery', 'waste_minimization', 
        'product_lifecycle', 'sustainability_metrics'
    ]
    '''
    user_prompt_topic = str('Please create a made-up research trend in the field of ' + research_field + '. The research trend should ideally be a composite term made up of two words that already exist, such as \"biolaser\", which does not exist as a term but \"bio\" and \"laser\" both exist and are commonly used. Bigrams can be expressed using a "_" like in "climate_change". The research trend should be described by a list of 10 words, where the first word is the trend itself and the other words are related terms, just like a topic resulting from topic modeling. The words should, however be very specific and unique to the new trend (LESS generic), so the topic could be distinguished very well from other topics within {research_field}. Please return only the word list of the topic in the form of [\"word1\" (research_trend_name), \"word2\", \"word3\", ..., \"word10\"]')
    artificial_topic = chatgpt(user_prompt = user_prompt_topic, model_name = "gpt-4o-mini")
    
    # Return the str object as list of str
    return eval(artificial_topic)


def generate_text(word_pool, target_length):
    '''
    Function to generate a synthetic text of a given target length using weighted sampling. 
    Words are randomly picked from a given word pool, that is a list of words, 
    where some words appear more often and others only once.
    Returns the generated text.

    Params:
     list(str): word_pool
     int: target_length
    
    Returns:
     str: text
    '''
    text = []

    # Pick words randomly from word pool until text is of desired length
    while len(text) < target_length:
        word = random.choice(word_pool)
        text.append(word)

    # Merge tokens together and return 
    return ' '.join(text)


def create_artificial_abstracts(t, artificial_topic, num_abstracts, num_freq_words = 100):
    '''
    Creates a given number of artificial abstracts (num_abstracts).
    Abstracts are generated by picking words from a word pool, 
    that consists of the top-10 words of a given topic (artificial_topic) and frequent words.
    The texts are adapted for a specific time t, as they pick the most frequent words from that dataset_t
    and their length is the mean abstract length for that subcorpus at t.
    Returns the generated abstracts in a list.

    Params:
     int: t
     list(str): artificial_topic
     int: num_abstracts
     int: num_freq_words
    
    Returns:
     list(str): artificial_abstracts
    '''
    # Load corpus file
    file_path = 'Data/data_for_t' + str(t) + '/corpus.tsv'

    # Function to tokenize the abstract lines
    def tokenize(text):
        return re.findall(r'\b\w+\b', text.lower())

    # Read corpus file to calculate word frequencies and mean abstract length
    with open(file_path, 'r') as file:
        lines = file.readlines()

    # Tokenizing all lines, counting word frequencies and abstract length
    word_counter = Counter()
    abstract_lengths = []
    for line in lines:
        tokens = tokenize(line)
        word_counter.update(tokens)
        abstract_lengths.append(len(tokens))  # Track the number of words in each abstract

    # Calculate the mean length of abstracts
    mean_abstract_length = int(sum(abstract_lengths) / len(abstract_lengths))

    # Extracting the most frequent words (number given as argument)
    most_freq_words = [word for word, _ in word_counter.most_common(num_freq_words)]

    # Get top 10 words of artificial ChatGPT topic
    topic_top_10 = artificial_topic

    # Step 2: Adjust probabilities to favor topic words more, with higher importance for earlier words
    # We assign a higher weight to topic words, with the most important word having the highest weight
    topic_weights = [10 - i for i in range(len(topic_top_10))]  # [10, 9, 8, ..., 1]
    frequent_word_weight = 1  # Frequent words have lower weight

    # Create a weighted pool of words to sample from
    word_pool = topic_top_10 * sum(topic_weights) + most_freq_words * frequent_word_weight
    for i, word in enumerate(topic_top_10):
        word_pool += [word] * topic_weights[i]

    # Generate the desired number of abstracts (e.g., 5, given as argument), each with the mean length of the corpus abstracts
    artificial_abstracts = [generate_text(word_pool, mean_abstract_length) for _ in range(num_abstracts)]

    return artificial_abstracts


def generate_dates(t, total_num_art_abstracts):
    '''
    Generates x artificial dates for a given time t.
    The number of dates to generate is given as a parameter (total_num_art_abstracts).
    The earliest date of t is calculated and from there on dates are generated within the t-interval.
    As t is a one year interval, it is divided in months.
    The number of dates is increasing for each month, following a growth function of 2x. 
    The first month has a intervals and a is depended on the total number of abstracts to create.
    The generated dates are returned in a list.

    Params:
     int: t
     int: total_num_art_abstracts
    
    Returns:
     list(date.time): date_list
    '''
    # Number of abstracts in the first month
    a = total_num_art_abstracts // 78

    # Find earliest date of total corpus
    corpus = pd.read_pickle('Data/corpus.pkl')
    earliest_date = corpus['coverDate'].min()

    # If str, convert to datetime
    if isinstance(earliest_date, str):
        date_format = '%Y-%m-%d'
        earliest_date = datetime.strptime(earliest_date, date_format)

    # Calculate start_date by adding t years to the earliest date of the corpus
    start_date_t = earliest_date.replace(year = earliest_date.year + (t-1))
    this_start_date = start_date_t

    # Loop over each month and generate dates (growing within the time frame)
    date_list = []
    for month in range(1, 13):
       
        # Number of abstracts for this month
        num_abstracts = a * month

        # Set end_date of this month (one month later)
        if this_start_date.month == 12:
            this_end_date = this_start_date.replace(year = this_start_date.year + 1).replace(month = 1) # if start is in December, end in January next year
        else:
            this_end_date = this_start_date.replace(month = this_start_date.month + 1)

        # Loop over abstracts and generate random date within this month
        for i in range(num_abstracts):
            random_date = this_start_date + timedelta(days = random.randint(0, (this_end_date - this_start_date).days - 1))

            # Append to the date to the result list
            date_list.append(random_date)
        # Set start date for next month as this month's end date
        this_start_date = this_end_date
    
    # Because of the integer division, there will be some abstracts without dates still (not enough dates, yet)
    # For the rest, a random date is picked within the 12 months:
    num_rest_dates = total_num_art_abstracts - len(date_list)
    for i in range(num_rest_dates):
        # Set end date to exactly one year after start date of t
        end_date_t = start_date_t.replace(year = start_date_t.year + 1)

        # Generate random_date within the whole 12 months
        random_date = start_date_t + timedelta(days = random.randint(0, (end_date_t - start_date_t).days - 1))

        # Append to the date to the result list
        date_list.append(random_date)
    
    return date_list


def create_artificial_data(t, artificial_topic, total_num_art_abstracts, num_freq_words = 100):
    '''
    Creates a dataframe with artificial data, generated by the functions defined above.

    Params:
     int: t
     list(str): artificial_topic
     int: total_num_art_abstracts
     int: num_freq_words

    Returns:
     pd.Dataframe
    '''
    # Generate the desired number of abstracts (given as argument) for a given time t and research field (as list)
    artificial_abstracts = pd.DataFrame(create_artificial_abstracts(t, artificial_topic, total_num_art_abstracts, num_freq_words), columns = ['text'])

    # Generate date list
    date_list = generate_dates(t, total_num_art_abstracts)
    
    # Turn into pandas series and concat with generated abstracts into result dataframe
    random_dates = pd.DataFrame(date_list, columns = ['date'])

    return pd.concat([artificial_abstracts, random_dates], axis = 1)    
